{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7hYqUsxYQtl"
      },
      "source": [
        "# (QLora) Fine-tuning Mistral-7b-Instruct to Respond to YouTube Comments\n",
        "\n",
        "Code authored by: Shaw Talebi <br>\n",
        "Video link: https://youtu.be/XpoKB3usmKc <br>\n",
        "Blog link: https://medium.com/towards-data-science/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32 <br>\n",
        "\n",
        "Colab link: https://colab.research.google.com/drive/1AErkPgDderPW0dgE230OOjEysd0QV1sR?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT-8d3vZYG4W"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeaOnjP2zsy-",
        "outputId": "74aa7624-4e96-4c69-e565-d5d081e3012e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.7.1-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
            "Collecting accelerate>=0.26.0 (from auto-gptq)\n",
            "  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting datasets (from auto-gptq)\n",
            "  Using cached datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting sentencepiece (from auto-gptq)\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Using cached rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.0.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from auto-gptq) (2.2.1)\n",
            "Collecting safetensors (from auto-gptq)\n",
            "  Downloading safetensors-0.4.2-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting transformers>=4.31.0 (from auto-gptq)\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "Collecting peft>=0.5.0 (from auto-gptq)\n",
            "  Using cached peft-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tqdm (from auto-gptq)\n",
            "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (24.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Collecting huggingface-hub (from accelerate>=0.26.0->auto-gptq)\n",
            "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
            "Collecting fsspec (from torch>=1.13.0->auto-gptq)\n",
            "  Using cached fsspec-2024.3.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.31.0->auto-gptq)\n",
            "  Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
            "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
            "     ---------------------------------------  41.0/42.0 kB 1.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  41.0/42.0 kB 1.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  41.0/42.0 kB 1.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  41.0/42.0 kB 1.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------  41.0/42.0 kB 1.9 MB/s eta 0:00:01\n",
            "     -------------------------------------- 42.0/42.0 kB 126.9 kB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.31.0->auto-gptq)\n",
            "  Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from tqdm->auto-gptq) (0.4.6)\n",
            "Collecting pyarrow>=12.0.0 (from datasets->auto-gptq)\n",
            "  Downloading pyarrow-15.0.1-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
            "Collecting pyarrow-hotfix (from datasets->auto-gptq)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto-gptq)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets->auto-gptq)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
            "Collecting xxhash (from datasets->auto-gptq)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets->auto-gptq)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=1.13.0->auto-gptq)\n",
            "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting aiohttp (from datasets->auto-gptq)\n",
            "  Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp->datasets->auto-gptq)\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->auto-gptq)\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.9.0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
            "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets->auto-gptq)\n",
            "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Downloading auto_gptq-0.7.1-cp310-cp310-win_amd64.whl (4.6 MB)\n",
            "   ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
            "   ------- -------------------------------- 0.9/4.6 MB 18.7 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 3.9/4.6 MB 41.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  4.6/4.6 MB 49.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  4.6/4.6 MB 49.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 4.6/4.6 MB 21.1 MB/s eta 0:00:00\n",
            "Using cached accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "Using cached peft-0.9.0-py3-none-any.whl (190 kB)\n",
            "Downloading safetensors-0.4.2-cp310-none-win_amd64.whl (269 kB)\n",
            "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
            "   --------------------------------------  266.2/269.5 kB 16.0 MB/s eta 0:00:01\n",
            "   --------------------------------------  266.2/269.5 kB 16.0 MB/s eta 0:00:01\n",
            "   --------------------------------------  266.2/269.5 kB 16.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 269.5/269.5 kB 1.7 MB/s eta 0:00:00\n",
            "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "Downloading gekko-1.0.7-py3-none-any.whl (13.1 MB)\n",
            "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.9/13.1 MB 19.4 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 1.9/13.1 MB 19.9 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 2.9/13.1 MB 20.2 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 3.9/13.1 MB 22.3 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 5.0/13.1 MB 21.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 6.2/13.1 MB 21.9 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 7.3/13.1 MB 22.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 8.5/13.1 MB 22.6 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 9.8/13.1 MB 23.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 11.2/13.1 MB 24.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.5/13.1 MB 25.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.1/13.1 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.1/13.1 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.1/13.1 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.1/13.1 MB 26.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 13.1/13.1 MB 17.7 MB/s eta 0:00:00\n",
            "Using cached rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
            "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
            "   --------------------------------------  983.0/991.5 kB 64.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  983.0/991.5 kB 64.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  983.0/991.5 kB 64.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  983.0/991.5 kB 64.9 MB/s eta 0:00:01\n",
            "   --------------------------------------  983.0/991.5 kB 64.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 991.5/991.5 kB 4.2 MB/s eta 0:00:00\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "Downloading aiohttp-3.9.3-cp310-cp310-win_amd64.whl (365 kB)\n",
            "   ---------------------------------------- 0.0/365.2 kB ? eta -:--:--\n",
            "   --------------------------------------  358.4/365.2 kB 23.2 MB/s eta 0:00:01\n",
            "   --------------------------------------  358.4/365.2 kB 23.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 365.2/365.2 kB 3.3 MB/s eta 0:00:00\n",
            "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
            "Downloading pyarrow-15.0.1-cp310-cp310-win_amd64.whl (24.8 MB)\n",
            "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 1.3/24.8 MB 42.0 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 2.8/24.8 MB 35.3 MB/s eta 0:00:01\n",
            "   ------ --------------------------------- 4.1/24.8 MB 32.4 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 5.7/24.8 MB 32.9 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 7.3/24.8 MB 33.2 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 8.9/24.8 MB 33.5 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 10.6/24.8 MB 32.8 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 12.2/24.8 MB 34.4 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 14.0/24.8 MB 34.4 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 15.8/24.8 MB 36.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 17.6/24.8 MB 36.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 19.5/24.8 MB 38.5 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 21.5/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 23.5/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.8/24.8 MB 40.9 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 24.8/24.8 MB 13.4 MB/s eta 0:00:00\n",
            "Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl (269 kB)\n",
            "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
            "   ---------------------------------------  266.2/269.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 269.5/269.5 kB 2.8 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.15.2-cp310-none-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ----------------------------------- ---- 2.0/2.2 MB 41.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.2/2.2 MB 46.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.2/2.2 MB 46.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 15.5 MB/s eta 0:00:00\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
            "   ---------------------------------------  133.1/134.8 kB ? eta -:--:--\n",
            "   ---------------------------------------  133.1/134.8 kB ? eta -:--:--\n",
            "   -------------------------------------- 134.8/134.8 kB 883.6 kB/s eta 0:00:00\n",
            "Downloading pandas-2.2.1-cp310-cp310-win_amd64.whl (11.6 MB)\n",
            "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
            "   ------- -------------------------------- 2.1/11.6 MB 44.5 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 4.0/11.6 MB 43.3 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 6.2/11.6 MB 43.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 8.4/11.6 MB 44.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.6/11.6 MB 43.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.6/11.6 MB 46.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.6/11.6 MB 46.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.6/11.6 MB 46.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.6/11.6 MB 28.5 MB/s eta 0:00:00\n",
            "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
            "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
            "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
            "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
            "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
            "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
            "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
            "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 50.4/50.4 kB 160.8 kB/s eta 0:00:00\n",
            "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
            "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
            "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
            "   ------------------------------------- -- 71.7/76.4 kB ? eta -:--:--\n",
            "   ------------------------------------- -- 71.7/76.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 76.4/76.4 kB 703.9 kB/s eta 0:00:00\n",
            "Installing collected packages: sentencepiece, pytz, xxhash, tzdata, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, multidict, gekko, fsspec, frozenlist, dill, attrs, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, accelerate, transformers, peft, datasets, auto-gptq\n",
            "Successfully installed accelerate-0.28.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 auto-gptq-0.7.1 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 gekko-1.0.7 huggingface-hub-0.21.4 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.1 peft-0.9.0 pyarrow-15.0.1 pyarrow-hotfix-0.6 pytz-2024.1 regex-2023.12.25 rouge-1.0.1 safetensors-0.4.2 sentencepiece-0.2.0 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.2 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting optimum\n",
            "  Using cached optimum-1.17.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting coloredlogs (from optimum)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from optimum) (1.12)\n",
            "Requirement already satisfied: transformers>=4.26.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from optimum) (2.2.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from optimum) (24.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from optimum) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from optimum) (0.21.4)\n",
            "Requirement already satisfied: datasets in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from optimum) (2.18.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (2024.2.0)\n",
            "Requirement already satisfied: requests in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.11->optimum) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch>=1.11->optimum) (3.1.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum) (0.4.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from transformers[sentencepiece]>=4.26.0->optimum) (0.2.0)\n",
            "Collecting protobuf (from transformers[sentencepiece]>=4.26.0->optimum)\n",
            "  Using cached protobuf-5.26.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (15.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (2.2.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from datasets->optimum) (3.9.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->optimum)\n",
            "  Using cached pyreadline3-3.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.8.0->optimum) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from pandas->datasets->optimum) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from pandas->datasets->optimum) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from pandas->datasets->optimum) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
            "Using cached optimum-1.17.1-py3-none-any.whl (407 kB)\n",
            "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Using cached protobuf-5.26.0-cp310-abi3-win_amd64.whl (420 kB)\n",
            "Using cached pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
            "Installing collected packages: pyreadline3, protobuf, humanfriendly, coloredlogs, optimum\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.17.1 protobuf-5.26.0 pyreadline3-3.4.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.43.0-py3-none-win_amd64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: torch in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from bitsandbytes) (2.2.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch->bitsandbytes) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch->bitsandbytes) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from torch->bitsandbytes) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Using cached bitsandbytes-0.43.0-py3-none-win_amd64.whl (101.6 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install auto-gptq\n",
        "%pip install optimum\n",
        "%pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tfSl5xRs0y8J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVstnrh-0m2x"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "88e1cf65d505401288505abe35a59f31",
            "4e0db07ef5ff43b98fcbdd399e70ff36",
            "621f7ed115864299a11a87127c32304e",
            "daecfbdbd7b949d689810561384e1fde",
            "e35dc7369d3e46319bdf56f1b08097a3",
            "795c2e9af11d404c92dc35c1184acece",
            "ce1b9589156b42a7a6effdbb274cd4da",
            "7ee13bdb1b264311864ffea2c94e9d38",
            "9f45ec20d88944f4825ca2402de85db0",
            "c3de7c54705f46b3944b60cf230317cf",
            "47ddb92cf4f94e009606d4684ace8789",
            "fffb54a547a24aacb74c316b013e8024",
            "2457090ecb084ecbbbb8db61d26ae1fd",
            "9e19a613838141de9301e54d2d3b8324",
            "fd37f51d4efb40db8a9509596d48015a",
            "4174275dae2341149b69bd193f79c34d",
            "812dfb653090454c94c7226990686625",
            "fc94a0dd65734f258b0de2ea876c9c85",
            "5b304595f6a8412c9ce5442725de530d",
            "0e9633a095de4643b9936206eccf44ac",
            "65472120905f4122aa4ed6ced86a4b5d",
            "83ace7a583bb41e6a13886f1cb0f7e07",
            "1869dfc09dfe4606beb053d01246ce77",
            "578dac09ef9b4e09a1a64c80f11ac3ed",
            "c419ce7315a54d58803047ab4fa82c89",
            "6a8517180b324c608c3661849ec651c4",
            "31366d3b17114485ad3762b950b8eb98",
            "184874f5df5344efb7c878cfac7c471a",
            "2082d6db2a734cd5b884985539abccfc",
            "4118c30f83b04be191eca36a8a43b072",
            "17312a3916e545f6b7398dd03ce7ed54",
            "79d683a91643419797ae7ba205dbea5f",
            "438453ccb47b4edaaf8c8e29c97aafeb"
          ]
        },
        "id": "GFcKal6c96su",
        "outputId": "76fff266-7c8b-491d-ddc3-a5d96029b7f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\notan\\.cache\\huggingface\\hub\\models--TheBloke--Mistral-7B-Instruct-v0.2-GPTQ. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\transformers\\modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
        "                                             trust_remote_code=False, # prevents running custom model files on your machine\n",
        "                                             revision=\"main\") # which version of model to use in repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCKC3_yl0pNS"
      },
      "source": [
        "### Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c3CIa8C80Vtm"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnFiyBOc329l"
      },
      "source": [
        "### Using Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Du9KLy3N8Ap6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "c:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST] Great content, thank you! [/INST] I'm glad you found the content helpful! If you have any specific questions or topics you'd like me to cover in the future, feel free to ask. I'm here to help.\n",
            "\n",
            "In the meantime, I'd be happy to answer any questions you have about the content I've already provided. Just let me know which article or blog post you're referring to, and I'll do my best to provide you with accurate and up-to-date information.\n",
            "\n",
            "Thanks for reading, and I look forward to helping you with any questions you may have!</s>\n"
          ]
        }
      ],
      "source": [
        "model.eval() # model in evaluation mode (dropout modules are deactivated)\n",
        "\n",
        "# craft prompt\n",
        "comment = \"Great content, thank you!\"\n",
        "prompt=f'''[INST] {comment} [/INST]'''\n",
        "\n",
        "# tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7XAi8rtC5a9"
      },
      "source": [
        "#### Prompt Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U77JPibZ4QuK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
            "\n",
            "Please respond to the following comment.\n",
            " \n",
            "Great content, thank you! \n",
            "[/INST]\n"
          ]
        }
      ],
      "source": [
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EUaf1YyZ7R7G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n",
            "\n",
            "Please respond to the following comment.\n",
            " \n",
            "Great content, thank you! \n",
            "[/INST] Thank you for your kind words! I'm glad you found the content helpful. –ShawGPT</s>\n"
          ]
        }
      ],
      "source": [
        "# tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXc8pBsVIy2N"
      },
      "source": [
        "### Prepare Model for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K3PesDIyI1Hn"
      },
      "outputs": [],
      "source": [
        "model.train() # model in training mode (dropout modules are activated)\n",
        "\n",
        "# enable gradient check pointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# enable quantized training\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iXOHXZcII5FS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7928519441906561\n"
          ]
        }
      ],
      "source": [
        "# LoRA config\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# LoRA trainable version of model\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# trainable parameter count\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tssCKXx3hUqM"
      },
      "source": [
        "### Preparing Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TqWGy2UBI8bo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading readme: 100%|██████████| 531/531 [00:00<?, ?B/s] \n",
            "Downloading data: 100%|██████████| 18.0k/18.0k [00:00<00:00, 63.6kB/s]\n",
            "Downloading data: 100%|██████████| 8.09k/8.09k [00:00<00:00, 32.2kB/s]\n",
            "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 123.67 examples/s]\n",
            "Generating test split: 100%|██████████| 9/9 [00:00<00:00, 1805.21 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "data = load_dataset(\"shawhin/shawgpt-youtube-comments\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D9LEA_g9gZPF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 50/50 [00:00<00:00, 121.32 examples/s]\n",
            "Map: 100%|██████████| 9/9 [00:00<00:00, 529.51 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# create tokenize function\n",
        "def tokenize_function(examples):\n",
        "    # extract text\n",
        "    text = examples[\"example\"]\n",
        "\n",
        "    #tokenize and truncate text\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "# tokenize training and validation datasets\n",
        "tokenized_data = data.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pow8yLjKcJM8"
      },
      "outputs": [],
      "source": [
        "# setting pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# data collator\n",
        "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-evbaTxhQTC"
      },
      "source": [
        "### Fine-tuning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e1PPLr4McNii"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "\n",
        "# define training arguments\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir= \"shawgptft\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    fp16=True,\n",
        "    # fp16=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m6wZK62bJKsJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "  0%|          | 0/30 [20:45<?, ?it/s]\n",
            "c:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "No inf checks were recorded for this optimizer.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# model.config.inference_mode = False\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# renable warnings\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\transformers\\trainer.py:2017\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2014\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m _grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2016\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[1;32m-> 2017\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2018\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\accelerate\\optimizer.py:136\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\notan\\miniconda3\\envs\\cs229-project\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:449\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mREADY:\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n",
            "\u001b[1;31mAssertionError\u001b[0m: No inf checks were recorded for this optimizer."
          ]
        }
      ],
      "source": [
        "# configure trainer\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# train model\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "# model.config.inference_mode = False\n",
        "trainer.train()\n",
        "\n",
        "# renable warnings\n",
        "model.config.use_cache = True\n",
        "# model.config.inference_mode = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3-3yynACoX0"
      },
      "source": [
        "### Push model to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c5KhuhpCno-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# # option 2: key login\n",
        "# from huggingface_hub import login\n",
        "# write_key = 'hf_' # paste token here\n",
        "# login(write_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYLbvvuPCq7q"
      },
      "outputs": [],
      "source": [
        "hf_name = 'shawhin' # your hf username or org name\n",
        "model_id = hf_name + \"/\" + \"shawgpt-ft\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4nAFjYPDC4_"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(model_id)\n",
        "trainer.push_to_hub(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDxRMA_BCBAt"
      },
      "source": [
        "### Load Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9gRgNjNiTvK"
      },
      "outputs": [],
      "source": [
        "# # load model from hub\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "# model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "#                                              device_map=\"auto\",\n",
        "#                                              trust_remote_code=False,\n",
        "#                                              revision=\"main\")\n",
        "\n",
        "# config = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\n",
        "# model = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n",
        "\n",
        "# # load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJIo4nUnhgiU"
      },
      "source": [
        "### Use Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITUJZDIYjs4t"
      },
      "outputs": [],
      "source": [
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
        "\n",
        "comment = \"Great content, thank you!\"\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9wNZ2URivBW"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1Pzx5q_wt2z"
      },
      "outputs": [],
      "source": [
        "comment = \"What is fat-tailedness?\"\n",
        "prompt = prompt_template(comment)\n",
        "\n",
        "model.eval()\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OybvrnR5Qy_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e9633a095de4643b9936206eccf44ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17312a3916e545f6b7398dd03ce7ed54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "184874f5df5344efb7c878cfac7c471a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1869dfc09dfe4606beb053d01246ce77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_578dac09ef9b4e09a1a64c80f11ac3ed",
              "IPY_MODEL_c419ce7315a54d58803047ab4fa82c89",
              "IPY_MODEL_6a8517180b324c608c3661849ec651c4"
            ],
            "layout": "IPY_MODEL_31366d3b17114485ad3762b950b8eb98"
          }
        },
        "2082d6db2a734cd5b884985539abccfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2457090ecb084ecbbbb8db61d26ae1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812dfb653090454c94c7226990686625",
            "placeholder": "​",
            "style": "IPY_MODEL_fc94a0dd65734f258b0de2ea876c9c85",
            "value": "model.safetensors: 100%"
          }
        },
        "31366d3b17114485ad3762b950b8eb98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4118c30f83b04be191eca36a8a43b072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4174275dae2341149b69bd193f79c34d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438453ccb47b4edaaf8c8e29c97aafeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47ddb92cf4f94e009606d4684ace8789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e0db07ef5ff43b98fcbdd399e70ff36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_795c2e9af11d404c92dc35c1184acece",
            "placeholder": "​",
            "style": "IPY_MODEL_ce1b9589156b42a7a6effdbb274cd4da",
            "value": "config.json: 100%"
          }
        },
        "578dac09ef9b4e09a1a64c80f11ac3ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_184874f5df5344efb7c878cfac7c471a",
            "placeholder": "​",
            "style": "IPY_MODEL_2082d6db2a734cd5b884985539abccfc",
            "value": "generation_config.json: 100%"
          }
        },
        "5b304595f6a8412c9ce5442725de530d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "621f7ed115864299a11a87127c32304e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ee13bdb1b264311864ffea2c94e9d38",
            "max": 1080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f45ec20d88944f4825ca2402de85db0",
            "value": 1080
          }
        },
        "65472120905f4122aa4ed6ced86a4b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a8517180b324c608c3661849ec651c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79d683a91643419797ae7ba205dbea5f",
            "placeholder": "​",
            "style": "IPY_MODEL_438453ccb47b4edaaf8c8e29c97aafeb",
            "value": " 111/111 [00:00&lt;00:00, 7.60kB/s]"
          }
        },
        "795c2e9af11d404c92dc35c1184acece": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79d683a91643419797ae7ba205dbea5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee13bdb1b264311864ffea2c94e9d38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812dfb653090454c94c7226990686625": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ace7a583bb41e6a13886f1cb0f7e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88e1cf65d505401288505abe35a59f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e0db07ef5ff43b98fcbdd399e70ff36",
              "IPY_MODEL_621f7ed115864299a11a87127c32304e",
              "IPY_MODEL_daecfbdbd7b949d689810561384e1fde"
            ],
            "layout": "IPY_MODEL_e35dc7369d3e46319bdf56f1b08097a3"
          }
        },
        "9e19a613838141de9301e54d2d3b8324": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b304595f6a8412c9ce5442725de530d",
            "max": 4158662280,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e9633a095de4643b9936206eccf44ac",
            "value": 4158662280
          }
        },
        "9f45ec20d88944f4825ca2402de85db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3de7c54705f46b3944b60cf230317cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c419ce7315a54d58803047ab4fa82c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4118c30f83b04be191eca36a8a43b072",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17312a3916e545f6b7398dd03ce7ed54",
            "value": 111
          }
        },
        "ce1b9589156b42a7a6effdbb274cd4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daecfbdbd7b949d689810561384e1fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3de7c54705f46b3944b60cf230317cf",
            "placeholder": "​",
            "style": "IPY_MODEL_47ddb92cf4f94e009606d4684ace8789",
            "value": " 1.08k/1.08k [00:00&lt;00:00, 66.2kB/s]"
          }
        },
        "e35dc7369d3e46319bdf56f1b08097a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc94a0dd65734f258b0de2ea876c9c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd37f51d4efb40db8a9509596d48015a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65472120905f4122aa4ed6ced86a4b5d",
            "placeholder": "​",
            "style": "IPY_MODEL_83ace7a583bb41e6a13886f1cb0f7e07",
            "value": " 4.16G/4.16G [00:38&lt;00:00, 136MB/s]"
          }
        },
        "fffb54a547a24aacb74c316b013e8024": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2457090ecb084ecbbbb8db61d26ae1fd",
              "IPY_MODEL_9e19a613838141de9301e54d2d3b8324",
              "IPY_MODEL_fd37f51d4efb40db8a9509596d48015a"
            ],
            "layout": "IPY_MODEL_4174275dae2341149b69bd193f79c34d"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
